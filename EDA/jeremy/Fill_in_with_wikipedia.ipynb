{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74eb519e",
   "metadata": {},
   "source": [
    "# Use Wikipedia as alternative data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f425a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "import re\n",
    "import timeit\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import bs4\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db553be",
   "metadata": {},
   "source": [
    "## Load Formated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a153c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = pd.read_pickle(\"../../data/Tables/country_df.pkl\")\n",
    "comes_from_df = pd.read_pickle(\"../../data/Tables/comes_from_df.pkl\")\n",
    "genre_df = pd.read_pickle(\"../../data/Tables/genre_df.pkl\")\n",
    "is_of_type_df = pd.read_pickle(\"../../data/Tables/is_of_type_df.pkl\")\n",
    "language_df = pd.read_pickle(\"../../data/Tables/language_df.pkl\")\n",
    "spoken_languages_df = pd.read_pickle(\"../../data/Tables/spoken_languages_df.pkl\")\n",
    "character_df = pd.read_pickle(\"../../data/Tables/character_df.pkl\")\n",
    "actor_df = pd.read_pickle(\"../../data/Tables/actor_df.pkl\")\n",
    "movie_df = pd.read_pickle(\"../../data/Tables/movie_df.pkl\")\n",
    "belongs_to_df = pd.read_pickle(\"../../data/Tables/belongs_to_df.pkl\")\n",
    "play_df = pd.read_pickle(\"../../data/Tables/play_df.pkl\")\n",
    "appears_in_df = pd.read_pickle(\"../../data/Tables/appears_in_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba5b1a5",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c49f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_movie_entries = set([\"Directed by\",\"Written by\",\"Produced by\",\"Starring\",\n",
    "                            \"Cinematography\",\"Edited by\",\"Music by\",\"Distributed by\",\n",
    "                            \"Release date\",\"Running time\",\"Country\",\"Language\",\n",
    "                            \"Budget\",\"Box office\",\"Screenplay by\",\"Based on\",\n",
    "                            \"Release dates\",\"Story by\",\"Director of Animation\",\n",
    "                            \"Languages\",\"Country of origin\",\"Original language\",\n",
    "                            \"Executive producer\",\"Producer\",\"Production\",\"Production company\",\n",
    "                            \"Distributor\",\"Original release\",\"Picture format\",\"Audio format\",\n",
    "                            \"Original network\",\"Release\",\"Editor\",\"Composer\", \"Countries\",\n",
    "                            \"Production locations\",\"Camera setup\"])\n",
    "\n",
    "def get_movie_data(page_id,entry_keys=default_movie_entries):\n",
    "    \"\"\" Grep data from wikipedia given the page_id of interest. \"\"\"\n",
    "    # We load and parse the main wikipedia page \n",
    "    page = wikipedia.page(pageid=page_id)\n",
    "    page_parser = BeautifulSoup(page.html(),\"html.parser\")\n",
    "    table_data = page_parser.find(\"table\",class_=\"infobox\")\n",
    "    # We extract the data relevent for our usage on movies\n",
    "    table_data_list = [s for s in list(table_data.descendants)[0].strings \n",
    "                       if re.match(\"\\A[(,),\\n,\\[,\\]]\",s) == None]\n",
    "    entry_indices = [idx for (idx,entry) in enumerate(table_data_list)\n",
    "                    if entry in entry_keys]\n",
    "    entry_indices.append(len(table_data_list))\n",
    "    table_data_dict = dict([(table_data_list[entry_indices[i]],\n",
    "                             table_data_list[entry_indices[i]+1:entry_indices[i+1]])\n",
    "                           for i in range(len(entry_indices)-1)])\n",
    "    table_data_dict[\"Summary\"] = page.summary\n",
    "    return table_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b924fd",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ff988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_wikipedia_data(movie_ids,wikipedia_data_dict,faulty_ids,\n",
    "                            entry_keys=default_movie_entries,verbose=False):\n",
    "    \"\"\" Pipeline that collect data from wikipedia for the given indices.\n",
    "    \n",
    "        It will add directly the information to the given lists. For the \n",
    "        ids where we cannot grep the wikipedia page we add them to the faulty_ids\n",
    "        list and otherwise we add the data to the wikipedia_data_dict dictionnary.\n",
    "        \n",
    "    \"\"\"\n",
    "    for idx in tqdm(movie_ids):\n",
    "        try:\n",
    "            wikipedia_data_dict[idx] = get_movie_data(idx,entry_keys=entry_keys)\n",
    "        except:\n",
    "            if verbose:\n",
    "                print(f\"Unable to fetch data for id {idx}.\")\n",
    "            faulty_ids.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496f648",
   "metadata": {},
   "source": [
    "#### Initialization of data containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c405328",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_data_dict = dict()\n",
    "faulty_ids = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b86196",
   "metadata": {},
   "source": [
    "#### Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de1e6e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b207d30752045e2a62da63fc2db67f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6902 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jerem\\anaconda3\\envs\\ada\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\jerem\\anaconda3\\envs\\ada\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "no_release_date_movie_ids =list(movie_df[movie_df[\"release_date\"].isna()].index)\n",
    "retrieve_wikipedia_data(no_release_date_movie_ids,wikipedia_data_dict,faulty_ids,\n",
    "                            entry_keys=default_movie_entries,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d705b4",
   "metadata": {},
   "source": [
    "#### Format dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f458f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnal if the goal is to obtain a flatten version of the output dictionnary.s\n",
    "wikipedia_data_list = []\n",
    "for idx, data_dict in wikipedia_data_dict.items():\n",
    "    new_data_dict = {\"movie_id\":idx}\n",
    "    new_data_dict.update(data_dict)\n",
    "    wikipedia_data_list.append(new_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa05b0b6",
   "metadata": {},
   "source": [
    "#### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aec9c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/Wikipedia/no_release_date_movies.json', 'w') as outfile:\n",
    "    json.dump(wikipedia_data_dict,outfile)\n",
    "\n",
    "with open('../../data/Wikipedia/faulty_no_release_date_movies.pkl', 'wb') as handle:\n",
    "    pickle.dump(faulty_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca2ca33",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
