{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970f123f",
   "metadata": {},
   "source": [
    "# Data Clustering\n",
    "\n",
    "We have high data dimensionality which can be an issue for ease of interpretability. Thus it would be convenient if we could group some features in diffent sub-categories. For example, cluster the genre in several different genre-representative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb7304",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "034dc71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b92f0",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a78070",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = pd.read_pickle(\"../../data/post_processing//country_df.pkl\")\n",
    "comes_from_df = pd.read_pickle(\"../../data/post_processing/comes_from_df.pkl\")\n",
    "genre_df = pd.read_pickle(\"../../data/post_processing/genre_df.pkl\")\n",
    "is_of_type_df = pd.read_pickle(\"../../data/post_processing/is_of_type_df.pkl\")\n",
    "language_df = pd.read_pickle(\"../../data/post_processing/language_df.pkl\")\n",
    "spoken_languages_df = pd.read_pickle(\"../../data/post_processing/spoken_languages_df.pkl\")\n",
    "character_df = pd.read_pickle(\"../../data/post_processing/character_df.pkl\")\n",
    "actor_df = pd.read_pickle(\"../../data/post_processing/actor_df.pkl\")\n",
    "movie_df = pd.read_pickle(\"../../data/post_processing/movie_df.pkl\")\n",
    "belongs_to_df = pd.read_pickle(\"../../data/post_processing/belongs_to_df.pkl\")\n",
    "play_df = pd.read_pickle(\"../../data/post_processing/play_df.pkl\")\n",
    "appears_in_df = pd.read_pickle(\"../../data/post_processing/appears_in_df.pkl\")\n",
    "wikipedia_imdb_mapping_table = pd.read_pickle(\"../../data/generated/wikipedia_imdb_mapping_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a54a56",
   "metadata": {},
   "source": [
    "## Genre Clustering\n",
    "\n",
    "BOW creation Pipeline:\n",
    "- casefolding\n",
    "- remove stopwords\n",
    "- add single words\n",
    "- add bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "368b29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENRE_ID_COL_NAME = \"genre_name\"\n",
    "MOVIE_ID_COL_NAME = \"movie_id\"\n",
    "custom_stopwords = {\"movie\",\"movies\",\"film\",\"films\",\"cinema\",\"&\",\"and\",\"in\",\"of\"}\n",
    "\n",
    "def add_bigrams(words_list: list)-> list:\n",
    "    \"\"\"\n",
    "    Append bigrams to the given words list.\n",
    "    \n",
    "    :param words_list: List words as strings.\n",
    "    \n",
    "    :return: Concatenated lists of both words and bigrams.\n",
    "    \n",
    "    \"\"\"\n",
    "    bigrams = [words_list[i]+\" \"+words_list[i+1] for i in range(len(words_list)-1)]\n",
    "    return words_list+bigrams\n",
    "\n",
    "def process_genre_name(name: str, stop_words: set) -> list:\n",
    "    \"\"\"\n",
    "    Apply basic processing steps to the genre name:\n",
    "        - Casefolding\n",
    "        - Stopwords removal\n",
    "        - Bigram addition\n",
    "        \n",
    "    :param name: String for the genre name\n",
    "    :param stop_words: Set of words considered as stopwords.\n",
    "    \n",
    "    :return: List of processed words and bigrams.\n",
    "    \n",
    "    \"\"\"\n",
    "    words_list = [w.casefold() for w in re.split(\"\\s|/|-\", name) if w not in stop_words]\n",
    "    words_with_bigrams = add_bigrams(words_list)\n",
    "    return words_with_bigrams\n",
    "\n",
    "def generate_vocabulary(genre_dataframe: pd.DataFrame, stop_words: set) -> set:\n",
    "    \"\"\"\n",
    "    Generate the vocabulary out of the given genre DataFrame.\n",
    "    \n",
    "    :param genre_dataframe: Pandas DataFrame containing the data for the genre names.\n",
    "    :param stop_words: Set of words considered as stopwords.\n",
    "    \n",
    "    :return: Set of words and bigrams contained in the vocabulary.\n",
    "    \n",
    "    \"\"\"\n",
    "    genre_name_df = genre_dataframe.reset_index()\n",
    "    genre_name_df[\"words\"] = genre_name_df[GENRE_ID_COL_NAME].apply(\n",
    "            lambda name: process_genre_name(name,stop_words))\n",
    "    vocabulary = set(genre_name_df[\"words\"].aggregate(sum))\n",
    "    return vocabulary\n",
    "\n",
    "def generate_BOW_matrix(movie_genre_dataframe: pd.DataFrame, genre_dataframe: pd.DataFrame,\n",
    "                        stop_words: set) -> tuple:\n",
    "    \"\"\"\n",
    "    Generate the BOW matrix using the following pipeline:\n",
    "        - Process genre names\n",
    "        - Create vocabulary\n",
    "        - Create BOW matrix \n",
    "        \n",
    "    :param movie_genre_dataframe: Pandas DataFrame containing the association between genres and movies.\n",
    "    :param genre_dataframe: Pandas DataFrame containing the data for the genre names.\n",
    "    :param stop_words: Set of words considered as stopwords.\n",
    "    \n",
    "    :return: Return the vocabulary, the ordered list of movie ids for the BOW matrix, and the BOW matrix.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create Vocabulary.\n",
    "    vocabulary = generate_vocabulary(genre_dataframe,stop_words)\n",
    "    # Create one-hot encoding of words for each genre-movie pair.\n",
    "    movie_genre_words_df = movie_genre_dataframe.copy()\n",
    "    movie_genre_words_df[GENRE_ID_COL_NAME] = movie_genre_words_df[GENRE_ID_COL_NAME].apply(\n",
    "            lambda name: process_genre_name(name,stop_words))\n",
    "    movie_genre_words_df[GENRE_ID_COL_NAME] = movie_genre_words_df[GENRE_ID_COL_NAME].apply(\n",
    "            lambda words_list: np.array([1 if w in set(words_list) else 0 for w in list(vocabulary)]))\n",
    "    # Aggregates by summation into a BOW representation the different movies.\n",
    "    grouped_movie_genre_df = movie_genre_words_df.groupby(MOVIE_ID_COL_NAME)[\n",
    "            GENRE_ID_COL_NAME].apply(sum)\n",
    "    grouped_movie_genre_df = grouped_movie_genre_df.sort_index()\n",
    "    # Convert pandas Series to numpy array.\n",
    "    BOW_matrix = grouped_movie_genre_df.reset_index()[GENRE_ID_COL_NAME].agg(\n",
    "        np.concatenate).reshape(len(grouped_movie_genre_df),len(vocabulary))\n",
    "    return vocabulary, grouped_movie_genre_df.index, BOW_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c7b9be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, movie_ids, BOW_matrix = generate_BOW_matrix(is_of_type_df, genre_df, custom_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6880048",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "51e2e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters=25)\n",
    "k_means.fit(BOW_matrix);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "1030b276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indie 0.054417122040072964\n",
      "family 0.058515482695810646\n",
      "romance 0.11725865209471772\n",
      "musical 0.12090163934426214\n",
      "comedy 1.0965391621129343\n",
      "\n",
      "\n",
      " name                                                Blazing Saddles\n",
      "release_date                                    1974-02-07 00:00:00\n",
      "revenue                                                 119500000.0\n",
      "runtime                                                        93.0\n",
      "freebase_id                                                /m/018f8\n",
      "plot              In the American Old West of 1874, construction...\n",
      "average_rating                                                  7.7\n",
      "num_votes                                                    140391\n",
      "Name: 3837, dtype: object\n",
      "\n",
      "\n",
      "         movie_id genre_name\n",
      "9252        3837     satire\n",
      "97853       3837    western\n",
      "114754      3837     comedy\n"
     ]
    }
   ],
   "source": [
    "movie_id = 3\n",
    "words_nb = 5\n",
    "topic_id = k_means.predict(BOW_matrix[movie_id,:].reshape(1,len(vocabulary)))[0]\n",
    "topic = k_means.cluster_centers_[topic_id,:]\n",
    "top_words_ids = np.argpartition(topic, -words_nb)[-words_nb:]\n",
    "for word_id in top_words_ids:\n",
    "    print(list(vocabulary)[word_id], topic[word_id])\n",
    "print(\"\\n\\n\",movie_df.loc[movie_ids[movie_id]])\n",
    "print(\"\\n\\n\",is_of_type_df[is_of_type_df[MOVIE_ID_COL_NAME] == movie_ids[movie_id]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e5cc78",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f8f7b3b1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [277], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m LDA \u001b[38;5;241m=\u001b[39m LatentDirichletAllocation(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m LDA_fit \u001b[38;5;241m=\u001b[39m LDA\u001b[38;5;241m.\u001b[39mfit(BOW_matrix)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\sklearn\\decomposition\\_lda.py:640\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_em_step(\n\u001b[0;32m    633\u001b[0m             X[idx_slice, :],\n\u001b[0;32m    634\u001b[0m             total_samples\u001b[38;5;241m=\u001b[39mn_samples,\n\u001b[0;32m    635\u001b[0m             batch_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    636\u001b[0m             parallel\u001b[38;5;241m=\u001b[39mparallel,\n\u001b[0;32m    637\u001b[0m         )\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;66;03m# batch update\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_em_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# check perplexity\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_every \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m evaluate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\sklearn\\decomposition\\_lda.py:503\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._em_step\u001b[1;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;124;03m\"\"\"EM update for 1 iteration.\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \n\u001b[0;32m    478\u001b[0m \u001b[38;5;124;03mupdate `_component` by batch VB or online VB.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;124;03m    Unnormalized document topic distribution.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;66;03m# E-step\u001b[39;00m\n\u001b[1;32m--> 503\u001b[0m _, suff_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# M-step\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_update:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\sklearn\\decomposition\\_lda.py:446\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._e_step\u001b[1;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 446\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_update_doc_distribution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp_dirichlet_component_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_topic_prior_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_doc_update_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_change_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;66;03m# merge result\u001b[39;00m\n\u001b[0;32m    460\u001b[0m doc_topics, sstats_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\joblib\\parallel.py:1048\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1049\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\joblib\\parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\joblib\\parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    781\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 782\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\sklearn\\decomposition\\_lda.py:136\u001b[0m, in \u001b[0;36m_update_doc_distribution\u001b[1;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cal_sstats:\n\u001b[0;32m    135\u001b[0m         norm_phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(exp_doc_topic_d, exp_topic_word_d) \u001b[38;5;241m+\u001b[39m EPS\n\u001b[1;32m--> 136\u001b[0m         suff_stats[:, ids] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mouter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_doc_topic_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnorm_phi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (doc_topic_distr, suff_stats)\n",
      "File \u001b[1;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mouter\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=10)\n",
    "LDA_fit = LDA.fit(BOW_matrix);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "5dba64ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romantic 1901.694903267021\n",
      "romance 2101.9050772361607\n",
      "comedy 14904.928566113716\n",
      "romantic comedy 2154.1648887839456\n",
      "horror 3518.7636553451184\n",
      "\n",
      "\n",
      " name                                               Army of Darkness\n",
      "release_date                                    1992-10-09 00:00:00\n",
      "revenue                                                  21502796.0\n",
      "runtime                                                        81.0\n",
      "freebase_id                                                /m/014hr\n",
      "plot              After being pulled through a time portal, Ash ...\n",
      "average_rating                                                  7.4\n",
      "num_votes                                                    179545\n",
      "Name: 3217, dtype: object\n",
      "\n",
      "\n",
      "         movie_id        genre_name\n",
      "1169        3217  action/adventure\n",
      "18098       3217       time travel\n",
      "18735       3217     costume drama\n",
      "44077       3217            comedy\n",
      "52564       3217       zombie film\n",
      "64511       3217            horror\n",
      "71105       3217      black comedy\n",
      "80667       3217     horror comedy\n",
      "81898       3217       stop motion\n",
      "120498      3217            action\n",
      "128837      3217           fantasy\n",
      "132953      3217              cult\n"
     ]
    }
   ],
   "source": [
    "words_nb = 5\n",
    "test_movie_id = 0\n",
    "test_movie = BOW_matrix[test_movie_id,:].reshape(1,len(vocabulary))\n",
    "topic = np.argmax(LDA_fit.transform(test_movie))\n",
    "top_words_ids = np.argpartition(LDA_fit.components_[topic,:], -words_nb)[-words_nb:]\n",
    "for word_id in top_words_ids:\n",
    "    print(list(vocabulary)[word_id], LDA_fit.components_[topic,word_id])\n",
    "print(\"\\n\\n\",movie_df.loc[movie_ids[test_movie_id]])\n",
    "print(\"\\n\\n\",is_of_type_df[is_of_type_df[MOVIE_ID_COL_NAME] == movie_ids[test_movie_id]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
