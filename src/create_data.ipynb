{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Formating Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "from utils import data_load\n",
    "from dateutil.parser import parse as parse_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling of CMU dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Metadata Extraction\n",
    "\n",
    "We extract here into dataframe the raw information from the CMU dataset. Note that we are merging the plot, when available, directly in the raw movie dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_movie_df = data_load.get_raw_movie_dataframe(\"../data/MovieSummaries/movie.metadata.tsv\",\"../data/MovieSummaries/plot_summaries.txt\")\n",
    "raw_character_df = data_load.get_raw_character_dataframe(\"../data/MovieSummaries/character.metadata.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables Creation\n",
    "\n",
    "We create now the different tables according to the ER diagram we have designed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Country table and relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df, comes_from_df = data_load.create_entry_and_relation_table(raw_movie_df,\"countries\",\n",
    "                                                           \"country_name\",\"movie_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Genre table and relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df, is_of_type_df = data_load.create_entry_and_relation_table(raw_movie_df,\"genres\",\n",
    "                                                           \"genre_name\",\"movie_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Language table and relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_df, spoken_languages_df = data_load.create_entry_and_relation_table(raw_movie_df,\"languages\",\"language_name\",\n",
    "                                                                  \"movie_id\",filter_dict={\" language\":\"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Character table\n",
    "\n",
    "We remove characters with no Freebase_id because none of them has a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_df = pd.DataFrame(raw_character_df[\"character_name\"][~raw_character_df.index.isna()])\n",
    "character_df = character_df[~character_df.index.duplicated()]\n",
    "character_df.index = character_df.index.rename(\"character_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actor table\n",
    "\n",
    "We will not keep actors and actresses that have a nan id, that are filled with only nan values or duplicated entries in the dataset.\n",
    "\n",
    "One example of such duplicated entries is Clark Kent and Superman that are two different characters, thus there will be two rows in the original CMU character df, but they are played by the same actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_df = raw_character_df[[\"actor_name\",\"actor_gender\",\n",
    "                                \"actor_height\",\"actor_ethnicity\",\"actor_birth_date\",\n",
    "                                \"freebase_actor_id\"]].set_index(\"freebase_actor_id\")\n",
    "actor_table_columns_mapping = {\"actor_birth_date\":\"birth_date\",\"actor_gender\":\"gender\",\n",
    "                                \"actor_height\":\"height\",\"actor_ethnicity\":\"ethnicity\",\n",
    "                                \"actor_name\":\"name\",\"freebase_actor_id\":\"actor_id\"}\n",
    "actor_df = actor_df[~actor_df.index.isna()]\n",
    "actor_df = actor_df[~actor_df.index.duplicated()].rename(\n",
    "                columns=actor_table_columns_mapping)\n",
    "actor_df.index = actor_df.index.rename(\"actor_id\")\n",
    "actor_df = actor_df.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Movie table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 8.44374% of the movie dataset with missing release date.\n"
     ]
    }
   ],
   "source": [
    "movie_df = raw_movie_df[[\"name\",\"release_date\",\"revenue\",\"runtime\",\"freebase_id\",\"plot\"]]\n",
    "movie_df.loc[movie_df[\"release_date\"] == \"1010-12-02\",\"release_date\"] = \"2010-12-02\"\n",
    "movie_df.index = raw_movie_df.index.rename(\"movie_id\")\n",
    "percentage_of_movies_with_missing_release_date = movie_df[\"release_date\"].isna().sum()/len(movie_df)\n",
    "print(f\"We have {100*percentage_of_movies_with_missing_release_date:.5f}% of the movie dataset with missing release date.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \"Belongs to\" table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "belongs_to_df = pd.DataFrame(\n",
    "    raw_character_df[\"wikipedia_movie_id\"][~raw_character_df.index.isna()])\n",
    "belongs_to_df = belongs_to_df.reset_index().drop_duplicates()\n",
    "# Convert back index to linear range\n",
    "belongs_to_df = belongs_to_df.reset_index()[[\"freebase_character_id\",\"wikipedia_movie_id\"]]\n",
    "belongs_to_table_columns_mapping = {\"freebase_character_id\":\"character_id\",\"wikipedia_movie_id\":\"movie_id\"}\n",
    "belongs_to_df = belongs_to_df.rename(columns=belongs_to_table_columns_mapping)\n",
    "belongs_to_df = belongs_to_df[belongs_to_df[\"character_id\"].isin(set(character_df.index))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \"Plays\" table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_df = pd.DataFrame(\n",
    "    raw_character_df[[\"freebase_actor_id\",\"freebase_map_id\"]][~raw_character_df.index.isna()])\n",
    "play_df = play_df.reset_index().drop_duplicates()\n",
    "# Convert back index to linear range\n",
    "play_df = play_df.reset_index()[[\"freebase_actor_id\",\"freebase_character_id\",\"freebase_map_id\"]]\n",
    "play_table_columns_mapping = {\"freebase_character_id\":\"character_id\",\n",
    "                                    \"freebase_actor_id\":\"actor_id\"}\n",
    "play_df = play_df.rename(columns=play_table_columns_mapping)\n",
    "play_df = play_df[play_df[\"actor_id\"].isin(set(actor_df.index))]\n",
    "play_df = play_df[play_df[\"character_id\"].isin(set(character_df.index))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \"Appears in\" table\n",
    "\n",
    "Remove duplicates that can appear in the dataset (same actor for clark kent and superman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "appears_in_df = raw_character_df.reset_index()[[\"freebase_actor_id\",\n",
    "    \"wikipedia_movie_id\",\"actor_age_at_release_date\"]].drop_duplicates()\n",
    "# Convert back index to linear range\n",
    "appears_in_table_columns_mapping = {\"wikipedia_movie_id\":\"movie_id\",\n",
    "                                    \"freebase_actor_id\":\"actor_id\",\n",
    "                                    \"actor_age_at_release_date\":\"actor_age\"}\n",
    "appears_in_df = appears_in_df.rename(columns=appears_in_table_columns_mapping)\n",
    "appears_in_df = appears_in_df[appears_in_df[\"actor_id\"].isin(set(actor_df.index))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out the duplicated actors and actresses\n",
    "\n",
    "In the dataset we have some duplicated actors and actresses. They have different freebase ids but have exactly the same attributes and are indeed duplicates when we look at the filmography. However, for many of such duplicates we do not have enough information to be assume with confidence that there are duplicates. Are two actors named John Bravo the same actors or not? It is hard to tell. Thus we decided that we tagged two actors entries as duplicates if they share the same name and same birthdate (the same birthyear is not consider as sufficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load.process_duplicated_actors(actor_df,[play_df,appears_in_df])\n",
    "play_df = play_df[~play_df[\"actor_id\"].isna()]\n",
    "appears_in_df = appears_in_df[~appears_in_df[\"actor_id\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration\n",
    "\n",
    "Our primary goal is to perform a time serie analysis of the different features we have in our movie dataset. The problem is that around 8.5% of the dataset is missing the release date entry. We will try to gather information from Wikipedia to recover the information and thus avoid to throw away this data.\n",
    "\n",
    "Furthermore, to extend our analysis, we are interested in other features linked to our data such as movie ratings or movie director, producer, etc. Thus we will try to retrieve some data from IMDB for our movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have the wikipedia data for 5205 movie with no release date in the CMU dataset.\n"
     ]
    }
   ],
   "source": [
    "wikipedia_data = pd.read_json(\"../data/Wikipedia/no_release_date_movies.json\").T\n",
    "print(f\"We have the wikipedia data for {len(wikipedia_data)} movie with no release date in the CMU dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load.parse_date_columns(wikipedia_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 719 movies for which we cannot retrieve the release date from the wikipedia data.\n"
     ]
    }
   ],
   "source": [
    "unparsable_date_movies = wikipedia_data[(wikipedia_data[\"Release dates\"] == \"\") \n",
    "               & (wikipedia_data[\"Release date\"] == \"\")\n",
    "               & (wikipedia_data[\"Original release\"] == \"\")]\n",
    "print(f\"There are {len(unparsable_date_movies)} movies for which we cannot retrieve the release date from the wikipedia data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_retrieved_dates = (wikipedia_data[\"Release dates\"]\n",
    "                             + wikipedia_data[\"Release date\"]\n",
    "                             + wikipedia_data[\"Original release\"])\n",
    "wikipedia_retrieved_dates = wikipedia_retrieved_dates[wikipedia_retrieved_dates != \"\"]\n",
    "wikipedia_retrieved_dates = pd.DataFrame(wikipedia_retrieved_dates,columns=[\"release_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.update(wikipedia_retrieved_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df[\"release_date\"] = pd.to_datetime(movie_df[\"release_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Data Integration\n",
    "\n",
    "We are interested in all the complementary information that we can retrieve using the IMDB database. To do so we will keep track of mapping between the wikipedia indices and the IMDB page indices. The mapping is not necessarly straightforward. We will first map on the name, but we may have duplicates as some movies have the same names. This method is not perfect also because we may have misspelled names. \n",
    "\n",
    "\n",
    "Thus we need to find a way to filter out duplicates that may occur. We will first conserve entries that share the same date of release. Then if it is not enough to remove all duplicates we will filter on the runtime. And if the latter is not sufficient, we do manual check to assign correctly the ids. This is possible as the number of movies at this stage is quite small. We initially tried with a filtering on the number of votes (i.e. keep the movie with the highest vote) based on the hypothesis that if a movie is more popular than another, then it is more likely to be in our dataset. But after verification with the data, this hypothesis does not hold everytime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load IMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_movie_df = data_load.load_imdb_title_basics()\n",
    "imdb_movie_df = imdb_movie_df[imdb_movie_df.title_type == \"movie\"].reset_index()\n",
    "imdb_movie_df.drop([\"index\",\"original_title\",\"title_type\",\"is_adult\",\"end_year\",\"genres\"], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge original information and IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we merge on the titles name (problem: names not unique and maybe spelling errors)\n",
    "merged = movie_df.reset_index().merge(\n",
    "    imdb_movie_df, left_on='name', right_on='primary_title', how='left', suffixes=('_cmu', '_imdb'))\n",
    "merged = merged[~merged[\"tconst\"].isna()]\n",
    "merged[\"release_date\"] = pd.to_datetime(merged[\"release_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Duplicates\n",
    "\n",
    "We call \"standard\" the data without duplicates and \"duplicated\" the one with duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_ids = data_load.get_duplicated_movie_ids(merged)\n",
    "merged_standard = merged[~duplicated_ids]\n",
    "merged_duplicated = merged[duplicated_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_duplicated = merged_duplicated[\n",
    "    merged_duplicated.release_date.apply(lambda r: r.year) == merged_duplicated.start_year]\n",
    "merged_standard, merged_duplicated = data_load.update_merged_dataframes(merged_standard,merged_duplicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_duplicated = merged_duplicated[merged_duplicated[\"runtime\"] == \n",
    "                                      merged_duplicated[\"runtime_minutes\"]]\n",
    "merged_standard, merged_duplicated = data_load.update_merged_dataframes(merged_standard,merged_duplicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_duplicated = merged_duplicated.drop(data_load.LAST_DUPLICATES_ID_LIST)\n",
    "merged_standard, merged_duplicated = data_load.update_merged_dataframes(merged_standard,merged_duplicated)\n",
    "assert len(merged_duplicated) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create mapping table\n",
    "\n",
    "Now that we have a fully merged table, we can drop the irrelevent columns and keep only the mapping between wikipedia movie ids, that we are using as general movie ids, and the IMDB ids.\n",
    "\n",
    "Note that we filter out movies that have no release dates (~500 movies) as we want to have a dataset that we can investigate in a temporal way without doing filtering. Also we filter out movies with the same IMDB id. This happen because some movies share the same name but are not necessarly both present in the IMDB dataset. Then the merge opperation caused both movie to be mapped to this IMDB id but it is most of the case a mistake, thus we filter them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_standard = merged_standard[~merged_standard[\"release_date\"].isna()]\n",
    "merged_standard = merged_standard[~merged_standard[\"tconst\"].isna()]\n",
    "merged_standard = merged_standard[~merged_standard[\"tconst\"].duplicated(keep=False)]\n",
    "wikipedia_imdb_mapping_table = merged_standard[[\"movie_id\",\"tconst\"]].set_index(\"movie_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df.to_pickle(\"../data/generated/country_df.pkl\")\n",
    "comes_from_df.to_pickle(\"../data/generated/comes_from_df.pkl\")\n",
    "genre_df.to_pickle(\"../data/generated/genre_df.pkl\")\n",
    "is_of_type_df.to_pickle(\"../data/generated/is_of_type_df.pkl\")\n",
    "language_df.to_pickle(\"../data/generated/language_df.pkl\")\n",
    "spoken_languages_df.to_pickle(\"../data/generated/spoken_languages_df.pkl\")\n",
    "character_df.to_pickle(\"../data/generated/character_df.pkl\")\n",
    "actor_df.to_pickle(\"../data/generated/actor_df.pkl\")\n",
    "movie_df.to_pickle(\"../data/generated/movie_df.pkl\")\n",
    "belongs_to_df.to_pickle(\"../data/generated/belongs_to_df.pkl\")\n",
    "play_df.to_pickle(\"../data/generated/play_df.pkl\")\n",
    "appears_in_df.to_pickle(\"../data/generated/appears_in_df.pkl\")\n",
    "wikipedia_imdb_mapping_table.to_pickle(\"../data/generated/wikipedia_imdb_mapping_df.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
